# -*- coding: utf-8 -*-
"""DM_DoAn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FSF7kiTXyyHcZJl7HXnAh-EwswA0W_lc
"""

import pandas as  pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
import pickle
data=pd.read_csv("/content/drive/My Drive/Colab Notebooks/diabetes_data_upload.csv")
#kiem tra null => Data kh√¥ng null!!
# print(data.isna().sum())
# Chia tap du lieu thanh 3 phan: train, validation va test
train_validation, test=train_test_split(data, test_size=0.2, stratify=data['class'], random_state=17)
train, validation=train_test_split(train_validation, test_size=0.2, stratify=train_validation['class'], random_state=17)
data=data.replace('Male', 1)
data=data.replace('Female', 0)
data=data.replace('Yes', 1)
data=data.replace('No', 0)
target='class'
y=data[target]
X=data.drop(columns=['class'])
# Ti le giua Positive va Negative
print(y.value_counts(normalize=True))
#------
from sklearn.neighbors import KNeighborsClassifier
model=KNeighborsClassifier(n_neighbors=5)
nFold=10
scores=cross_val_score(model, X, y, cv=nFold)
print("Do chinh xac cua mo hinh su dung K-Neighbors voi nghi thuc kiem tra %d-fold %.3f" %(nFold, np.mean(scores)))
#------
# import category_encoders as ce
# # !pip install category_encoders
# from sklearn.pipeline import make_pipeline
# from sklearn.impute import SimpleImputer
# from sklearn.linear_model import LogisticRegression
# # from sklearn.metrics import roc_auc_score
# pipepline=make_pipeline(ce.OrdinalEncoder(), SimpleImputer(strategy='median'), LogisticRegression(random_state=100, max_iter=1000))
# model1=pipepline.fit(X,y)
# scores=cross_val_score(model1, X, y, cv=nFold)
# print("kiem tra %d-fold %.3f" %(nFold, np.mean(scores)))
#------
from sklearn.tree import DecisionTreeClassifier
decisionTreeModel=[]
criterion=['gini', 'entropy']
for i in criterion:
  model2=DecisionTreeClassifier(criterion=i, max_depth=10, max_features='auto', random_state=1, splitter='best')
  model2.fit(X, y)
  decisionTreeModel.append(model2)
  scores = cross_val_score(model2, X, y, cv=nFold)
  print("Do chinh xac cua mo hinh su dung Decision Tree tham so %s voi nghi thuc kiem tra %d-fold %.3f" %(i, nFold, np.mean(scores)))
pickle.dump(decisionTreeModel[0], open('model.pkl', 'wb'))

"""This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.

For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's.
https://medium.com/dev-genius/early-stage-diabetes-risk-prediction-bd42f113a20b
"""